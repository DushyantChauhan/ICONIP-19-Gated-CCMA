## Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis
Code for the paper [Multi-task Gated Contextual Cross-Modal Attention Framework for Sentiment and Emotion Analysis](https://link.springer.com/chapter/10.1007/978-3-030-36808-1_72) (ICONIP 2019)

For the evaluation of our proposed multi-task gated CCMA framerwork, we use benchmark multi-modal dataset i.e, MOSEI which has both sentiment and emotion classes.

### Dataset

* You can download datasets from [here](https://drive.google.com/open?id=1s10Bvmb7mInYof_Aui9y8q29dKmxYiB1).

* Download the dataset from given link and set the path in the code accordingly make two folder (i) results and (ii) weights.

-------------------------------------------------------
### For MOSEI Dataset:
for trimodal-->>  python trimodal_gated_multitask.py  

-------------------------------------------------------

### --versions--

python: 2.7

keras: 2.2.2

tensorflow: 1.9.0
